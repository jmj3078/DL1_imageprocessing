{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42CRdtcCRTuc"
      },
      "source": [
        "# Assignment 1 – Polynomial Regression using ``torch.nn.Module``\n",
        "\n",
        "- Please create a copy of this notebook onto your own Drive before working on it: `File-->Save a copy in Drive`\n",
        "- Please submit your ipynb file named with your initials, e.g. `KJH-Assignment1.ipynb` (or a URL link to it) with **the CODE cells output visible** to support your answers and **TEXTUAL answers given as comments** in the code cells.\n",
        "- Marks will be deducted for missing or partial code cell output where applicable.\n",
        "- Deadline for submission is **9:00am, Saturday, March 25th.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KvsIo2UfwO3"
      },
      "source": [
        "## Neural Network Model for Polynomial Regression\n",
        "Your task is to build **TWO different neural network models** for the function $y = x^2 + 5x$\n",
        "\n",
        "Requirements:\n",
        "- You MUST use `torch.nn.Module` to define your neural network classes.\n",
        "- A random seed is provided to help with code reproducibility so the code will always produce the same result each time you run it.\n",
        "- The training data should have **10 input values, $x$, and the correct corresponding output values, $y$,** for the function $y = x^2 + 5x$ \n",
        "- Each NN may have **maximum TWO hidden layers**.\n",
        "- You may use a **maximum of 500 neuron units in each hidden layer**.\n",
        "- You may train over a **maximum of 1000 epochs**.\n",
        "- Use suitable activation functions that have been covered in class. \n",
        "  - Note: Activation function should be used in every layer and the output layer depending on the type of output.\n",
        "- You MUST use the **Adam optimiser,** available as **`torch.optim.Adam()`** and the **mean squared error (MSE) loss function**.\n",
        "- **IMPORTANT:** **Your best 2 models should have a loss lower than 0.01** at the end of training.\n",
        "- If the loss value is the same, you can fill it out regardless of the rank.\n",
        "- Please write down **the top 2 models** with the best performance in terms of **lowest loss** from your experiment, including **the number of layers, hidden sizes, learning rate, epochs, and the final loss after running all epochs.**\n",
        "- Print the loss at every 50th epoch. \n",
        "- Test the model on $x=10$.\n",
        "- **Save your training loss** at every iteration.\n",
        "\n",
        "Note:\n",
        "- If your model does not achieve a loss of less than 0.01, you will still be awarded marks for `Q7 – Q10` as long as you can explain your answers accordingly.\n",
        "- For `Q7 – Q12`, please respond with the cases where the loss is the lowest."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 1. Define training data for a the mathematical formula y = x^2 + 5x (3)\n",
        "import numpy as np\n",
        "x_train = np.linspace(-10, 10, 20)\n",
        "y_train = x_train**2 + x_train*5\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "print(x_train)\n",
        "print(y_train)"
      ],
      "metadata": {
        "id": "ek4MzzzQYbdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e683eb-a9fb-4664-c379-59131b628a4a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-10.0000],\n",
            "        [ -8.9474],\n",
            "        [ -7.8947],\n",
            "        [ -6.8421],\n",
            "        [ -5.7895],\n",
            "        [ -4.7368],\n",
            "        [ -3.6842],\n",
            "        [ -2.6316],\n",
            "        [ -1.5789],\n",
            "        [ -0.5263],\n",
            "        [  0.5263],\n",
            "        [  1.5789],\n",
            "        [  2.6316],\n",
            "        [  3.6842],\n",
            "        [  4.7368],\n",
            "        [  5.7895],\n",
            "        [  6.8421],\n",
            "        [  7.8947],\n",
            "        [  8.9474],\n",
            "        [ 10.0000]])\n",
            "tensor([[ 50.0000],\n",
            "        [ 35.3186],\n",
            "        [ 22.8532],\n",
            "        [ 12.6039],\n",
            "        [  4.5706],\n",
            "        [ -1.2465],\n",
            "        [ -4.8476],\n",
            "        [ -6.2327],\n",
            "        [ -5.4017],\n",
            "        [ -2.3546],\n",
            "        [  2.9086],\n",
            "        [ 10.3878],\n",
            "        [ 20.0831],\n",
            "        [ 31.9945],\n",
            "        [ 46.1219],\n",
            "        [ 62.4654],\n",
            "        [ 81.0249],\n",
            "        [101.8006],\n",
            "        [124.7922],\n",
            "        [150.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdiTwoHS93b3"
      },
      "source": [
        "# Find out the 1st, 2nd lowest loss model\n",
        "torch.manual_seed(40)\n",
        "class PolynomialRegression_2_hidden(nn.Module):\n",
        "    \n",
        "    # two hidden layers\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(1, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, 1)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear3(x)\n",
        "        return x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolynomialRegression_1_hidden(nn.Module):\n",
        "    \n",
        "    # one hidden layer\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(1, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, 1)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WJSOvU7ui8_z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(40)\n",
        "num_epochs = [100, 300, 500, 700, 900, 1100]\n",
        "hidden_sizes = [100, 200, 300, 400, 500]\n",
        "\n",
        "# 1 hidden layer\n",
        "\n",
        "def PolynomialRegression_loop_1(num_epochs, hidden_size, learning_rate=0.1):\n",
        "    model = PolynomialRegression_1_hidden(hidden_size)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        y_pred = model(x_train)\n",
        "        loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    print(f'final loss :{loss.item():.5f}, epochs : {num_epochs}, hidden size : {hidden_size}, learning rate : {learning_rate}, hidden layer : 1')\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "temp = np.array([[0,0,0,0]], dtype=np.float32)\n",
        "for epochs in num_epochs : \n",
        "    for size in hidden_sizes :\n",
        "        row_add = [[epochs, size, PolynomialRegression_loop_1(epochs, size),1]]\n",
        "        temp = np.r_[temp, row_add]\n",
        "\n",
        "results_1 = np.delete(temp, 0, axis=0)\n",
        "\n",
        "# 2 hidden layer\n",
        "\n",
        "def PolynomialRegression_loop_2(num_epochs, hidden_size, learning_rate=0.1):\n",
        "    model = PolynomialRegression_2_hidden(hidden_size)\n",
        "\n",
        "    loss_fn = nn.MSELoss()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        y_pred = model(x_train)\n",
        "        loss = loss_fn(y_pred, y_train)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    \n",
        "    print(f'final loss :{loss.item():.5f}, epochs : {num_epochs}, hidden size : {hidden_size}, learning rate : {learning_rate}, hidden layer : 2')\n",
        "    return loss.item()\n",
        "\n",
        "temp = np.array([[0,0,0,0]], dtype=np.float32)\n",
        "for epochs in num_epochs : \n",
        "    for size in hidden_sizes :\n",
        "        row_add = [[epochs, size, PolynomialRegression_loop_2(epochs, size), 2]]\n",
        "        temp = np.r_[temp, row_add]\n",
        "\n",
        "results_2 = np.delete(temp, 0, axis=0)"
      ],
      "metadata": {
        "id": "kS5vRSTH7NcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9d92af-e28e-4759-9ee2-a1ee58c9325a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final loss :7.38780, epochs : 100, hidden size : 100, learning rate : 0.1, hidden layer : 1\n",
            "final loss :3.35172, epochs : 100, hidden size : 200, learning rate : 0.1, hidden layer : 1\n",
            "final loss :1.51168, epochs : 100, hidden size : 300, learning rate : 0.1, hidden layer : 1\n",
            "final loss :1.58870, epochs : 100, hidden size : 400, learning rate : 0.1, hidden layer : 1\n",
            "final loss :1.60491, epochs : 100, hidden size : 500, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.14957, epochs : 300, hidden size : 100, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.04223, epochs : 300, hidden size : 200, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.04896, epochs : 300, hidden size : 300, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.06867, epochs : 300, hidden size : 400, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.07489, epochs : 300, hidden size : 500, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.03667, epochs : 500, hidden size : 100, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.41108, epochs : 500, hidden size : 200, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.01895, epochs : 500, hidden size : 300, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.02451, epochs : 500, hidden size : 400, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.01863, epochs : 500, hidden size : 500, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.00813, epochs : 700, hidden size : 100, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.01075, epochs : 700, hidden size : 200, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.00526, epochs : 700, hidden size : 300, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.00183, epochs : 700, hidden size : 400, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.00481, epochs : 700, hidden size : 500, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.02109, epochs : 900, hidden size : 100, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.30954, epochs : 900, hidden size : 200, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.00104, epochs : 900, hidden size : 300, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.00249, epochs : 900, hidden size : 400, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.12054, epochs : 900, hidden size : 500, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.22815, epochs : 1100, hidden size : 100, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.04899, epochs : 1100, hidden size : 200, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.06474, epochs : 1100, hidden size : 300, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.01277, epochs : 1100, hidden size : 400, learning rate : 0.1, hidden layer : 1\n",
            "final loss :0.12147, epochs : 1100, hidden size : 500, learning rate : 0.1, hidden layer : 1\n",
            "final loss :7.54080, epochs : 100, hidden size : 100, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.44246, epochs : 100, hidden size : 200, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.20274, epochs : 100, hidden size : 300, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.13590, epochs : 100, hidden size : 400, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.66494, epochs : 100, hidden size : 500, learning rate : 0.1, hidden layer : 2\n",
            "final loss :181.14281, epochs : 300, hidden size : 100, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.03770, epochs : 300, hidden size : 200, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.01228, epochs : 300, hidden size : 300, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.06680, epochs : 300, hidden size : 400, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.00756, epochs : 300, hidden size : 500, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.21660, epochs : 500, hidden size : 100, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.01705, epochs : 500, hidden size : 200, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.14814, epochs : 500, hidden size : 300, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.00042, epochs : 500, hidden size : 400, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.00227, epochs : 500, hidden size : 500, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.16364, epochs : 700, hidden size : 100, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.00026, epochs : 700, hidden size : 200, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.00029, epochs : 700, hidden size : 300, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.00006, epochs : 700, hidden size : 400, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.00567, epochs : 700, hidden size : 500, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.42687, epochs : 900, hidden size : 100, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.04462, epochs : 900, hidden size : 200, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.00304, epochs : 900, hidden size : 300, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.00006, epochs : 900, hidden size : 400, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.00185, epochs : 900, hidden size : 500, learning rate : 0.1, hidden layer : 2\n",
            "final loss :1.36453, epochs : 1100, hidden size : 100, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.00087, epochs : 1100, hidden size : 200, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.83964, epochs : 1100, hidden size : 300, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.00002, epochs : 1100, hidden size : 400, learning rate : 0.1, hidden layer : 2\n",
            "final loss :0.00006, epochs : 1100, hidden size : 500, learning rate : 0.1, hidden layer : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = np.concatenate((results_1, results_2), axis=0)\n",
        "top1_2 = results[results[:,2].argsort()][:2, :]\n",
        "print(f\"<top1> epoch : {top1_2[0,0]}, hidden_size :{top1_2[0,1]}, loss : {top1_2[0,2]:.5f}, hidden layer : {top1_2[0,3]}\\n<top2> epoch : {top1_2[1,0]}, hidden_size :{top1_2[1,1]}, loss : {top1_2[1,2]:.5f}, hidden layer : {top1_2[1,3]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF-6bgXGo350",
        "outputId": "4157af79-45d0-4cfe-9621-e3f09a5b7ca0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<top1> epoch : 1100.0, hidden_size :400.0, loss : 0.00002, hidden layer : 2.0\n",
            "<top2> epoch : 1100.0, hidden_size :500.0, loss : 0.00006, hidden layer : 2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 1 Accuracy Model (the model that has the lowest loss)\n",
        "# <top1> epoch : 1100.0, hidden_size :400.0, loss : 0.00002, hidden layer : 2.0\n",
        "torch.manual_seed(40) # This is for reproducibility. You need NOT adjust this.\n",
        "\n",
        "# 2. Define NN class 1 (10)\n",
        "class PolynomialRegression_2_hidden(nn.Module):\n",
        "    \n",
        "    # two hidden layers\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(1, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, 1)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "\n",
        "# 3. Create an instance of NN model and define the hidden_size (2)\n",
        "hidden_size = 400\n",
        "model_top1 = PolynomialRegression_2_hidden(hidden_size)\n",
        "# 4. Loss and Optimiser (2)\n",
        "loss_fn = nn.MSELoss()\n",
        "opt = torch.optim.Adam(model_top1.parameters(), lr = 0.1)\n",
        "# 5. Training loop\n",
        "  # 5.1 Forward pass (2)\n",
        "num_epochs = 1100\n",
        "train_loss_1st = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    y_pred = model_top1(x_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        " # 5.2 Backward pass (3)\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "  # 5.3 Print loss every 50th epoch (1)\n",
        "    if (epoch+1) % 50 == 0 :\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss = {loss.item():.4f} ')\n",
        "  # 5.4 Save training loss at every epoch (2)\n",
        "    train_loss_1st.append(loss.item())\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh45Vi-tAXJi",
        "outputId": "cb3c501d-dc0e-48a0-be3b-c711a5cb44e9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/1100, Loss = 9.1176 \n",
            "Epoch 100/1100, Loss = 1.3345 \n",
            "Epoch 150/1100, Loss = 0.4965 \n",
            "Epoch 200/1100, Loss = 0.2467 \n",
            "Epoch 250/1100, Loss = 0.1808 \n",
            "Epoch 300/1100, Loss = 0.0687 \n",
            "Epoch 350/1100, Loss = 0.0354 \n",
            "Epoch 400/1100, Loss = 0.0208 \n",
            "Epoch 450/1100, Loss = 0.0131 \n",
            "Epoch 500/1100, Loss = 0.0084 \n",
            "Epoch 550/1100, Loss = 0.0055 \n",
            "Epoch 600/1100, Loss = 0.0035 \n",
            "Epoch 650/1100, Loss = 0.0024 \n",
            "Epoch 700/1100, Loss = 0.0016 \n",
            "Epoch 750/1100, Loss = 0.0012 \n",
            "Epoch 800/1100, Loss = 0.0009 \n",
            "Epoch 850/1100, Loss = 0.0007 \n",
            "Epoch 900/1100, Loss = 0.0006 \n",
            "Epoch 950/1100, Loss = 0.0004 \n",
            "Epoch 1000/1100, Loss = 0.0003 \n",
            "Epoch 1050/1100, Loss = 0.0002 \n",
            "Epoch 1100/1100, Loss = 0.0002 \n",
            "0.00017993853543885052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 2 Accuracy Model (the model that has the second lowest loss)\n",
        "# <top2> epoch : 1100.0, hidden_size :500.0, loss : 0.00006, hidden layer : 2.0\n",
        "torch.manual_seed(40) # This is for reproducibility. You need NOT adjust this.\n",
        "\n",
        "# 2. Define NN class 1 (10)\n",
        "class PolynomialRegression_2_hidden(nn.Module):\n",
        "    \n",
        "    # two hidden layers\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(1, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, 1)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "\n",
        "# 3. Create an instance of NN model and define the hidden_size (2)\n",
        "hidden_size = 500\n",
        "model_top2 = PolynomialRegression_2_hidden(hidden_size)\n",
        "# 4. Loss and Optimiser (2)\n",
        "loss_fn = nn.MSELoss()\n",
        "opt = torch.optim.Adam(model_top2.parameters(), lr = 0.1)\n",
        "# 5. Training loop\n",
        "  # 5.1 Forward pass (2)\n",
        "num_epochs = 1100\n",
        "train_loss_2nd = []\n",
        "for epoch in range(num_epochs):\n",
        "    y_pred = model_top2(x_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        " # 5.2 Backward pass (3)\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "  # 5.3 Print loss every 50th epoch (1)\n",
        "    if (epoch+1) % 50 == 0 :\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss = {loss.item():.4f} ')\n",
        "  # 5.4 Save training loss at every epoch (2)\n",
        "    train_loss_2nd.append(loss.item())\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "-LlBqYYpKdou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9af0303-fe9e-470b-f85f-353cec6d6f23"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/1100, Loss = 5.3857 \n",
            "Epoch 100/1100, Loss = 0.5602 \n",
            "Epoch 150/1100, Loss = 0.1084 \n",
            "Epoch 200/1100, Loss = 0.0418 \n",
            "Epoch 250/1100, Loss = 0.0251 \n",
            "Epoch 300/1100, Loss = 0.0167 \n",
            "Epoch 350/1100, Loss = 0.0111 \n",
            "Epoch 400/1100, Loss = 0.0073 \n",
            "Epoch 450/1100, Loss = 0.0049 \n",
            "Epoch 500/1100, Loss = 0.0034 \n",
            "Epoch 550/1100, Loss = 0.0025 \n",
            "Epoch 600/1100, Loss = 0.0019 \n",
            "Epoch 650/1100, Loss = 0.0015 \n",
            "Epoch 700/1100, Loss = 0.0012 \n",
            "Epoch 750/1100, Loss = 0.0010 \n",
            "Epoch 800/1100, Loss = 0.0008 \n",
            "Epoch 850/1100, Loss = 0.1199 \n",
            "Epoch 900/1100, Loss = 0.1093 \n",
            "Epoch 950/1100, Loss = 0.0432 \n",
            "Epoch 1000/1100, Loss = 0.0233 \n",
            "Epoch 1050/1100, Loss = 0.0126 \n",
            "Epoch 1100/1100, Loss = 0.0067 \n",
            "0.0066515496000647545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6. Please write down the top 2 models with the best performance in terms of lowest loss from your experiment. (3)\n",
        "# Including the number of layers, hidden sizes, learning rate, epochs, and the final loss after running all epochs.\n",
        "# Top 1\n",
        "# number of layers: 4 (1 input layer, 2 hidden layer, 1 output layer)\n",
        "# number of hidden sizes: 400\n",
        "# learning rate: 0.1\n",
        "# epochs: 1100\n",
        "# final loss: 0.00002\n",
        "\n",
        "# Top 2:\n",
        "# number of layers: 4 (1 input layer, 2 hidden layer, 1 output layer)\n",
        "# number of hidden sizes: 500\n",
        "# learning rate: 0.2\n",
        "# epochs: 1100\n",
        "# final loss: 0.00006"
      ],
      "metadata": {
        "id": "dpVsWM1gKARA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Q7-Q12, please respond using the best model (the model with the lowest loss)."
      ],
      "metadata": {
        "id": "JRDul8uFKEOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7. Visualize (3)\n",
        "# Plot the landscape of your training loss (MSE loss) saved for every epoch.\n",
        "# y-axis would mean MSE loss and x-axis would mean the epoch of your training.\n",
        "# Hint: you should plot (1,first MSE loss), ... ,(last epoch number,last MSE loss)\n",
        "import matplotlib.pyplot as plt\n",
        "x = [i for i in range(1, len(train_loss_1st)+1)]\n",
        "\n",
        "plt.title(\"Loss over training epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"MSE loss\")\n",
        "plt.plot(x, train_loss_1st)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xXWhlxrmLBSl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "7a3ef39e-8e30-445e-cf31-091796666721"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEWCAYAAACqitpwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcu0lEQVR4nO3debgnVX3n8fdHmkUWWVtk00bpYJAnImkRxkyiYtiSEf8wCjHSIpFkglGjz0QwZjAmJppxgpJRRiagEBU0RmMPQRERNCYj2qBBFgktQugOSAvNIiqyfOePOrfvj7txu2/97sb79Ty/p6tOnao6dQvu59Y59atKVSFJUp+eNNcNkCQtPoaLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGi7QAJfl8kpV9153vkrwzycfmuh16fEvmugF6YktyC/DbVfWluW7LbElSwPKqWrO526iqo4dRV+qLVy7SkCTZrD/eNnc9aT4xXDQvJdk6yfuT/Ef7vD/J1m3ZbkkuSnJPkruT/FOSJ7Vlb0uyLsn9SW5Mcvgk298xyflJ1ie5Nck7kjyp7feeJAcO1F2a5CdJntrmfz3Jt1u9f0nyCwN1b2ltuAZ4YGxQJPlqm/zXJD9K8qokL0qytq13B/CRJDu3Y1yfZEOb3ntgO1ck+e02/dokX0vyvlb3+0mO3sy6+yb5avv5fSnJB6fqhprGz+K0JNe3fX0kyTYDy1+fZE07h6uS7Dmw7DlJLm3LfpDk7QO73aqdu/uTXJdkxWTt09wxXDRf/RFwKHAQ8FzgEOAdbdlbgbXAUmB34O1AJdkfeAPw/KraATgSuGWS7f81sCPwTOBXgBOAE6vqQeAzwPEDdV8JfKWq7kzyPOBc4HeAXYEPA6tGgq85Hvg1YKeqenhwp1X1y23yuVW1fVV9ss0/DdgFeAZwMt3/mx9p808HfgL8r8l/XLwAuBHYDfhL4Jwk2Yy6nwC+0Y7tncBrJtvhNH8Wr6Y7D88Cfo52DpO8BPgLup/tHsCtwIVt2Q7Al4AvAHsC+wGXDWzzZa3uTsAqpv65aK5UlR8/c/ah++X/0gnKvwccMzB/JHBLm34X8DlgvzHr7AfcCbwU2HKKfW4B/Aw4YKDsd4Ar2vRLge8NLPtn4IQ2fRbwp2O2dyPwKwPH87rHOeYabDvwotaebaZY5yBgw8D8FXRjVQCvBdYMLNu27eNpm1KXLsQeBrYdWP4x4GOTtGk6P4vfHVh2zMjPFTgH+MuBZdsDDwHL6ML5W5Ps853AlwbmDwB+Mtf/HfsZ//HKRfPVnnR/zY64tZUB/A9gDfDFJDcnORWgugHyN9P9ArozyYWDXS0DdgO2nGD7e7Xpy4Ftk7wgyTK6X+yfbcueAby1dQPdk+QeYJ+BtgHctqkHC6yvqp+OzCTZNsmHW5fdfcBXgZ2SbDHJ+neMTFTVj9vk9ptYd0/g7oEymPpYNvVnMXgOH3N+q+pHwF1052Afuj8uJnPHwPSPgW3Gdj9q7hkumq/+g+6X14intzKq6v6qemtVPZOui+QtI2MrVfWJqvqltm4B751g2z+k+yt57PbXtW08AnyK7i/o44GLqur+Vu824N1VtdPAZ9uqumBgW5vzqPGx67wV2B94QVU9BRjpTpusq6sPtwO7JNl2oGyfKepP52cxuP7Gc8iY85tkO7qutXVtu8+cwXFoHjBcNB9smWSbgc8S4ALgHW0wfTfgv9N10YwMIu/XxgnuBR4BHk2yf5KXtD7/n9KNUzw6dmcD4fHuJDskeQbwlpHtN58AXkU3ZvCJgfL/A/xuu6pJku2S/FobJ5iuH/D4vzx3aO2/J8kuwOmbsP3NUlW3AquBdybZKslhwH+ZYpXp/CxOSbJ3O4Y/AkbGmC4ATkxyUDtffw5cWVW3ABcBeyR5c7obLHZI8oKeD1dDZrhoPriY7hfpyOedwJ/R/aK7BvgOcHUrA1hON+D7I+D/AR+qqsuBrYH30F2Z3AE8FThtkn3+PvAAcDPwNboAOXdkYVVd2ZbvCXx+oHw18Hq6QeQNdN1zr93E430ncF7rSnrlJHXeDzy5HcvX6Qa3Z8OrgcPouqj+jC4MHpyo4jR/Fp8Avkj3c/5e2ybVfa/pj4G/p7tiehZwXFt2P/CrdMF2B3AT8OJ+Dk+zJVW+LEzSxJJ8EvhuVW3ylVOegF+Q1SivXCRtlOT5SZ6V7js/RwHHAv8wx83SAuQdFpIGPY3uez670n2X6L9W1bfmtklaiOwWkyT1zm4xSVLv7BZrdtttt1q2bNlcN0OSFpSrrrrqh1W1dGy54dIsW7aM1atXz3UzJGlBSXLrROV2i0mSeme4SJJ6Z7hIknpnuEiSeme4SJJ6Z7hIknpnuEiSeme4zNCGB37GP15z+1w3Q5LmFcNlhn7v41dzyieu5vZ7fzLXTZGkecNwmaF193Sh8rOHx73wUJKesAyXnvhwaUkaZbhIknpnuPQkmesWSNL8YbhIknpnuEiSeme4SJJ6Z7hIkno31HBJ8gdJrktybZILkmyTZN8kVyZZk+STSbZqdbdu82va8mUD2zmtld+Y5MiB8qNa2Zokpw6UT7gPSdLsGFq4JNkLeCOwoqoOBLYAjgPeC5xRVfsBG4CT2ionARta+RmtHkkOaOs9BzgK+FCSLZJsAXwQOBo4ADi+1WWKfUiSZsGwu8WWAE9OsgTYFrgdeAnw6bb8PODlbfrYNk9bfniStPILq+rBqvo+sAY4pH3WVNXNVfUz4ELg2LbOZPuQJM2CoYVLVa0D3gf8O12o3AtcBdxTVQ+3amuBvdr0XsBtbd2HW/1dB8vHrDNZ+a5T7OMxkpycZHWS1evXr9/8g5UkPcYwu8V2prvq2BfYE9iOrltr3qiqs6tqRVWtWLp06Vw3R5IWjWF2i70U+H5Vra+qh4DPAC8EdmrdZAB7A+va9DpgH4C2fEfgrsHyMetMVn7XFPuQJM2CYYbLvwOHJtm2jYMcDlwPXA68otVZCXyuTa9q87TlX66qauXHtbvJ9gWWA98Avgksb3eGbUU36L+qrTPZPiRJs2CYYy5X0g2qXw18p+3rbOBtwFuSrKEbHzmnrXIOsGsrfwtwatvOdcCn6ILpC8ApVfVIG1N5A3AJcAPwqVaXKfYhSZoFSx6/yuarqtOB08cU30x3p9fYuj8FfmOS7bwbePcE5RcDF09QPuE+JEmzw2/oS5J6Z7hIknpnuEiSeme4SJJ6Z7hIknpnuEiSeme4SJJ6Z7hIknpnuEiSeme49KRqrlsgSfOH4SJJ6p3h0pNkrlsgSfOH4SJJ6p3hIknqneEiSeqd4SJJ6p3hIknqneEiSeqd4SJJ6p3hIknqneHSEx//IkmjDBdJUu8Ml574+BdJGmW49MRuMUkaZbhIknpnuPTEbjFJGmW4SJJ6Z7hIknpnuEiSeme4SJJ6Z7hIknpnuPTE77lI0ijDRZLUO8NFktQ7w6Un9opJ0ijDRZLUO8NFktQ7w6Un5e1ikrTRUMMlyU5JPp3ku0luSHJYkl2SXJrkpvbvzq1ukpyZZE2Sa5IcPLCdla3+TUlWDpT/YpLvtHXOTLrHR062D0nS7Bj2lcsHgC9U1bOB5wI3AKcCl1XVcuCyNg9wNLC8fU4GzoIuKIDTgRcAhwCnD4TFWcDrB9Y7qpVPtg9J0iwYWrgk2RH4ZeAcgKr6WVXdAxwLnNeqnQe8vE0fC5xfna8DOyXZAzgSuLSq7q6qDcClwFFt2VOq6uvV9UmdP2ZbE+1jaOwUk6RRw7xy2RdYD3wkybeS/E2S7YDdq+r2VucOYPc2vRdw28D6a1vZVOVrJyhnin08RpKTk6xOsnr9+vWbc4ySpAkMM1yWAAcDZ1XV84AHGNM91a44hvpH/1T7qKqzq2pFVa1YunTpDPczo9UlaVEZZrisBdZW1ZVt/tN0YfOD1qVF+/fOtnwdsM/A+nu3sqnK956gnCn2IUmaBUMLl6q6A7gtyf6t6HDgemAVMHLH10rgc216FXBCu2vsUODe1rV1CXBEkp3bQP4RwCVt2X1JDm13iZ0wZlsT7UOSNAuWDHn7vw98PMlWwM3AiXSB9qkkJwG3Aq9sdS8GjgHWAD9udamqu5P8KfDNVu9dVXV3m/494KPAk4HPtw/AeybZxxDZLyZJI4YaLlX1bWDFBIsOn6BuAadMsp1zgXMnKF8NHDhB+V0T7UOSNDv8hr4kqXeGS0+8W0ySRhkukqTeGS6SpN4ZLj2xV0ySRhkukqTeGS49cUBfkkYZLpKk3hkukqTeGS49KYf0JWkjw0WS1DvDRZLUO8OlJ94tJkmjDBdJUu82KVySPCnJU4bVGEnS4vC44ZLkE0mekmQ74Frg+iT/bfhNW1jsFpOkUdO5cjmgqu4DXk73psd9gdcMs1GSpIVtOuGyZZIt6cJlVVU9hM9pHMfvuUjSqOmEy4eBW4DtgK8meQZw3zAbJUla2JY8XoWqOhM4c6Do1iQvHl6TJEkL3XQG9N/UBvST5JwkVwMvmYW2LSgO6EvSqOl0i72uDegfAexMN5j/nqG2SpK0oE0nXNL+PQb426q6bqBMkqRxphMuVyX5Il24XJJkB+DR4TZLkrSQPe6APnAScBBwc1X9OMmuwIlDbZUkaUGbzt1ijybZG/jNJABfqar/O/SWSZIWrOncLfYe4E3A9e3zxiR/PuyGLTTeLSZJo6bTLXYMcFBVPQqQ5DzgW8Dbh9kwSdLCNd2nIu80ML3jENqx4Pn4F0kaNZ0rl78AvpXkcrpbkH8ZOHWorZIkLWjTGdC/IMkVwPNb0duq6o6htkqStKBNGi5JDh5TtLb9u2eSPavq6uE1a+FxQF+SRk115fI/p1hW+HwxSdIkJg2XqvLJx5KkzTLdu8X0OOwVk6RRhoskqXeGiySpd5OGS5LfGph+4Zhlb5juDpJskeRbSS5q8/smuTLJmiSfTLJVK9+6za9py5cNbOO0Vn5jkiMHyo9qZWuSnDpQPuE+hqm8XUySNprqyuUtA9N/PWbZ6zZhH28CbhiYfy9wRlXtB2yge+oy7d8NrfyMVo8kBwDHAc8BjgI+1AJrC+CDwNHAAcDxre5U+5AkzYKpwiWTTE80P/EGuqcp/xrwN20+dLcwf7pVOQ94eZs+ts3Tlh/e6h8LXFhVD1bV94E1wCHts6aqbq6qnwEXAsc+zj6GxusWSRo1VbjUJNMTzU/m/cAfMvpysV2Be6rq4Ta/FtirTe8F3AbQlt/b6m8sH7POZOVT7eMxkpycZHWS1evXr5/mIY3dxmatJkmL2lTh8uwk1yT5zsD0yPz+j7fhJL8O3FlVV/XV2L5V1dlVtaKqVixdunQzt9FzoyRpEZjqG/o/P8NtvxB4WZJjgG2ApwAfAHZKsqRdWewNrGv11wH7AGuTLKF7+vJdA+UjBteZqPyuKfYxNIaMJI2a9Mqlqm4d/AA/Ag4GdmvzU6qq06pq76paRjcg/+WqejVwOfCKVm0l8Lk2varN05Z/ubpbsFYBx7W7yfYFlgPfAL4JLG93hm3V9rGqrTPZPiRJs2CqW5EvSnJgm94DuJbuLrG/TfLmGezzbcBbkqyhGx85p5WfA+zayt9Ce6x/VV0HfIruLZhfAE6pqkfaVckbgEvo7kb7VKs71T4kSbNgqm6xfavq2jZ9InBpVZ2QZAfgn+kG66elqq4ArmjTN9Pd6TW2zk+B35hk/XcD756g/GLg4gnKJ9zHcNkvJkkjphrQf2hg+nDaL/Gqup/Ru78kSRpnqiuX25L8Pt2tvAfTdUmR5MnAlrPQNknSAjXVlctJdN+Kfy3wqqq6p5UfCnxkuM1aeLxbTJJGTfU+lzuB352g/HK6u7EkSZrQVK85XjXVilX1sv6bs3B54SJJo6YaczmM7vEqFwBXMs3niT3R+PgXSRpvqnB5GvCrwPHAbwL/CFww8F0S4ViLJE1kqm/oP1JVX6iqlXSD+GuAKzblXS5PJIaMJI2a6sqFJFvTPTL/eGAZcCbw2eE3S5K0kE01oH8+cCDdlyf/ZODb+pIkTWmqK5ffAh6ge5PkGzM6ch2gquopQ27bguJrjiVp1FTfc5nqC5aSJE3KAJEk9c5w6YmdYpI0ynCRJPXOcOmJ4/mSNMpwmSEf/yJJ4xkuM+QViySNZ7j0pBzSl6SNDJcZsltMksYzXGbIbjFJGs9w6YshI0kbGS6SpN4ZLpKk3hkuPbFXTJJGGS6SpN4ZLpKk3hkuPfGWZEkaZbhIknpnuPTEx79I0ijDZYZ8/IskjWe4zJBjLZI0nuHSE0NGkkYZLpKk3hkukqTeGS49sVdMkkYZLpKk3g0tXJLsk+TyJNcnuS7Jm1r5LkkuTXJT+3fnVp4kZyZZk+SaJAcPbGtlq39TkpUD5b+Y5DttnTOT7sbgyfYhSZodw7xyeRh4a1UdABwKnJLkAOBU4LKqWg5c1uYBjgaWt8/JwFnQBQVwOvAC4BDg9IGwOAt4/cB6R7XyyfYxNOXtYpK00dDCpapur6qr2/T9wA3AXsCxwHmt2nnAy9v0scD51fk6sFOSPYAjgUur6u6q2gBcChzVlj2lqr5e3W/288dsa6J9SJJmwayMuSRZBjwPuBLYvapub4vuAHZv03sBtw2straVTVW+doJyptjH0HjdIkmjhh4uSbYH/h54c1XdN7isXXEM9ffyVPtIcnKS1UlWr1+/frO27+NfJGm8oYZLki3pguXjVfWZVvyD1qVF+/fOVr4O2Gdg9b1b2VTle09QPtU+HqOqzq6qFVW1YunSpZt1jA61SNJ4w7xbLMA5wA1V9VcDi1YBI3d8rQQ+N1B+Qrtr7FDg3ta1dQlwRJKd20D+EcAlbdl9SQ5t+zphzLYm2sfwGDKStNGSIW77hcBrgO8k+XYrezvwHuBTSU4CbgVe2ZZdDBwDrAF+DJwIUFV3J/lT4Jut3ruq6u42/XvAR4EnA59vH6bYR+/sFpOk8YYWLlX1NWCyX72HT1C/gFMm2da5wLkTlK8GDpyg/K6J9jEMdotJ0nh+Q78nvixMkkYZLpKk3hkukqTeGS49cexFkkYZLpKk3hkuPfHKRZJGGS4z5PdcJGk8w2WGvGKRpPEMl56YMZI0ynCZIbvFJGk8w2WG7BaTpPEMl574mmNJGmW4zJDdYpI0nuEyQ16wSNJ4hktPzBhJGmW4SJJ6Z7j0xO4xSRpluMyQA/qSNJ7hMkNesUjSeIZLb0wZSRphuMyQ3WKSNJ7hMkN2i0nSeIZLTwwZSRpluMyQ3WKSNJ7hMkNesUjSeIZLT8wYSRpluEiSeme49MTuMUkaZbjMkAP6kjSe4TJDI1cs1/3HvXPbEEmaRwyXnnzoiu/NdRMkad4wXGbIbjFJGs9wmaHBgfxyVF+SAMOlV/c/+PBcN0GS5gXDZYYeeXT0auWeBx6aw5ZI0vxhuMzQgw8/snH67h//bA5bIknzh+EyQw8+9OjG6Q2GiyQBhsuMPfjwQLg8YLhIEizicElyVJIbk6xJcups7POatffy6KPeMSZJizJckmwBfBA4GjgAOD7JAcPY12HP2nXj9Ef/5RZWfuQbfP+HDwxjV5K0YCyZ6wYMySHAmqq6GSDJhcCxwPV972jX7bbixfsv5aY7f8TaDT/hn276IS9+3xXsvO2WbLXkSWyz5RY8aZJvWk76/ctJFkz1fc1s6j4kqTln5fN5+q7b9rrNxRouewG3DcyvBV4wtlKSk4GTAZ7+9Kdv1o7+6lUHAfBvP7ifdRt+wrdvu4cC1t//II8+Wvz04UcmfGLyZJ1nk30Rc8rOtkkWlm+ZkTQNWy3pvxNrsYbLtFTV2cDZACtWrJjRb+Kf230Hfm73HXjxs5/aS9skaSFblGMuwDpgn4H5vVuZJGkWLNZw+SawPMm+SbYCjgNWzXGbJOkJY1F2i1XVw0neAFwCbAGcW1XXzXGzJOkJY1GGC0BVXQxcPNftkKQnosXaLSZJmkOGiySpd4aLJKl3hoskqXfx1bydJOuBWzdz9d2AH/bYnPlkMR8bLO7j89gWroV0fM+oqqVjCw2XHiRZXVUr5rodw7CYjw0W9/F5bAvXYjg+u8UkSb0zXCRJvTNc+nH2XDdgiBbzscHiPj6PbeFa8MfnmIskqXdeuUiSeme4SJJ6Z7jMQJKjktyYZE2SU+e6PZsqyT5JLk9yfZLrkryple+S5NIkN7V/d27lSXJmO95rkhw8t0cwPUm2SPKtJBe1+X2TXNmO45PttQwk2brNr2nLl81pwx9Hkp2SfDrJd5PckOSwxXTukvxB++/y2iQXJNlmoZ67JOcmuTPJtQNlm3yukqxs9W9KsnIujmW6DJfNlGQL4IPA0cABwPFJDpjbVm2yh4G3VtUBwKHAKe0YTgUuq6rlwGVtHrpjXd4+JwNnzX6TN8ubgBsG5t8LnFFV+wEbgJNa+UnAhlZ+Rqs3n30A+EJVPRt4Lt0xLopzl2Qv4I3Aiqo6kO7VGcexcM/dR4GjxpRt0rlKsgtwOt0r2w8BTh8JpHmpqvxsxgc4DLhkYP404LS5btcMj+lzwK8CNwJ7tLI9gBvb9IeB4wfqb6w3Xz90byG9DHgJcBEQum8+Lxl7Hune/3NYm17S6mWuj2GS49oR+P7Y9i2WcwfsBdwG7NLOxUXAkQv53AHLgGs391wBxwMfHih/TL359vHKZfON/Mc/Ym0rW5BaN8LzgCuB3avq9rboDmD3Nr0Qj/n9wB8Cj7b5XYF7qurhNj94DBuPry2/t9Wfj/YF1gMfaV1+f5NkOxbJuauqdcD7gH8Hbqc7F1exOM7diE09VwvqHBouIsn2wN8Db66q+waXVfcn0oK8Xz3JrwN3VtVVc92WIVgCHAycVVXPAx5gtFsFWPDnbmfgWLoQ3RPYjvHdSovGQj5XkzFcNt86YJ+B+b1b2YKSZEu6YPl4VX2mFf8gyR5t+R7Ana18oR3zC4GXJbkFuJCua+wDwE5JRt7COngMG4+vLd8RuGs2G7wJ1gJrq+rKNv9purBZLOfupcD3q2p9VT0EfIbufC6GczdiU8/VgjqHhsvm+yawvN29shXdYOOqOW7TJkkS4Bzghqr6q4FFq4CRO1FW0o3FjJSf0O5mORS4d+Cyft6pqtOqau+qWkZ3fr5cVa8GLgde0aqNPb6R435Fqz8v/5qsqjuA25Ls34oOB65nkZw7uu6wQ5Ns2/47HTm+BX/uBmzquboEOCLJzu3K7ohWNj/N9aDPQv4AxwD/BnwP+KO5bs9mtP+X6C7FrwG+3T7H0PVVXwbcBHwJ2KXVD90dct8DvkN3J8+cH8c0j/VFwEVt+pnAN4A1wN8BW7fybdr8mrb8mXPd7sc5poOA1e38/QOw82I6d8CfAN8FrgX+Fth6oZ474AK6saOH6K46T9qccwW8rh3jGuDEuT6uqT4+/kWS1Du7xSRJvTNcJEm9M1wkSb0zXCRJvTNcJEm9M1ykRSDJi0ae+izNB4aLJKl3hos0i5L8VpJvJPl2kg+3d838KMkZ7d0llyVZ2uoelOTr7Z0enx1438d+Sb6U5F+TXJ3kWW3z22f0/S4fb99sl+aE4SLNkiQ/D7wKeGFVHQQ8Arya7qGMq6vqOcBX6N7ZAXA+8Laq+gW6b2qPlH8c+GBVPRf4T3Tf/IbuqdZvpnu/0DPpnsUlzYklj19FUk8OB34R+Ga7qHgy3cMKHwU+2ep8DPhMkh2BnarqK638PODvkuwA7FVVnwWoqp8CtO19o6rWtvlv070/5GtDPyppAoaLNHsCnFdVpz2mMPnjMfU295lMDw5MP4L/f2sO2S0mzZ7LgFckeSpsfIf6M+j+Pxx50u9vAl+rqnuBDUn+cyt/DfCVqrofWJvk5W0bWyfZdjYPQpoO/7KRZklVXZ/kHcAXkzyJ7gm5p9C96OuQtuxOunEZ6B7D/r9beNwMnNjKXwN8OMm72jZ+YxYPQ5oWn4oszbEkP6qq7ee6HVKf7BaTJPXOKxdJUu+8cpEk9c5wkST1znCRJPXOcJEk9c5wkST17v8D7F3AxjBCuuMAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zatxfhYejeB8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b25f729-f38c-45f6-f253-29eead74e854"
      },
      "source": [
        "# Q8. Prediction (2)\n",
        "# Let's use the model on a new number x, defined as a tensor\n",
        "# Get the model's prediction for this new x\n",
        "def test_model(test_num):\n",
        "    test = torch.tensor([test_num], dtype=torch.float32)\n",
        "    pred = model_top1(test).item()\n",
        "    print(f'Model Prediction for x = {test_num} : {pred:.4f}\\nerror :{((test_num**2 + 5*test_num) - pred):.4f}')\n",
        "\n",
        "test_model(10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Prediction for x = 10 : 149.9901\n",
            "error :0.0099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvmHgY1oDLiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2fcb2c5-51bf-4345-9e36-bf09bdeee207"
      },
      "source": [
        "# Make sure the output of your code cells support your answers below:\n",
        "\n",
        "# Q9. Describe how the loss changed over time during training. (2)\n",
        "\n",
        "# The training loss value decreased sharply, then rose slightly, and then continued to decrease again, showing a pattern of convergence.\n",
        "\n",
        "# Q10. Is the prediction for x=10 close enough to the ideal value of 150? \n",
        "\n",
        "# Why do you think the prediction is or isn't close enough to the ideal value? (2)\n",
        "# Yes, the prediction results was 150.0000, which is ideal value.\n",
        "\n",
        "# Q11. What are the predictions for x=20 and x=100? Based on these predictions, \n",
        "# comment on whether the model has captured the relationship between the training inputs and outputs. (2)\n",
        "\n",
        "test_model(20)\n",
        "# Model Prediction for x = 20 : 369.2545\n",
        "# error :130.7455\n",
        "test_model(100)\n",
        "# Model Prediction for x = 100 : 2140.8533\n",
        "# error :8359.1467\n",
        "\n",
        "# Since the range of training data is set between -10 and 10, \n",
        "# it was confirmed that good prediction results were not shown in the case of 20 and 100, which are far outside the range of training data.\n",
        "\n",
        "test_model(6)\n",
        "# Model Prediction for x = 6 : 66.0611\n",
        "# error :-0.0611\n",
        "test_model(-10)\n",
        "# Model Prediction for x = -10 : 50.0000\n",
        "# error :0.0000\n",
        "test_model(3)\n",
        "# Model Prediction for x = 3 : 23.6960\n",
        "# error :0.3040\n",
        "\n",
        "# However, It can be seen that values within the training data range almost completely predict. \n",
        "# Therefore, the relationship between the training input value and the output value seems to be well reflected.\n",
        "\n",
        "# Q12. Apart from tweaking the number of epochs and the number of neuron units in the hidden layer, think\n",
        "# of AT LEAST ONE more thing you would do to try to improve the model. You do NOT have to follow the \n",
        "# requirements nor to implement anything. (1)\n",
        "\n",
        "# 1. More training data with a wider range, It is possible to train a model that can explain a wider range.\n",
        "# 2. Using different type of layers, such as convolutional layers or recurrent layers\n",
        "# 3. Different activation function, such as sigmoid or tanh functions. "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Prediction for x = 20 : 389.6318\n",
            "error :110.3682\n",
            "Model Prediction for x = 100 : 2324.2954\n",
            "error :8175.7046\n",
            "Model Prediction for x = 6 : 66.0282\n",
            "error :-0.0282\n",
            "Model Prediction for x = -10 : 50.0000\n",
            "error :-0.0000\n",
            "Model Prediction for x = 3 : 23.9839\n",
            "error :0.0161\n"
          ]
        }
      ]
    }
  ]
}