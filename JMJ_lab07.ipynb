{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D3dotnhNCKsl"
      },
      "source": [
        "## Lab 7 (Image Processing using Convolutional Neural Networks)\n",
        "- CIFAR10 dataset (see https://www.cs.toronto.edu/~kriz/cifar.html for more info)\n",
        "- 60K images: 50K train, 10K test\n",
        "- 10 classes: 'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "- Perform multi-class classification with evaluation accuracy on EACH class\n",
        "\n",
        "**CONNECT TO GPU** before continuing, but just CPU is also fine, it might be a bit slow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ8bPH1OCM5h",
        "outputId": "dcd890b7-9d2e-4bc7-a14e-8a063c5a011e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [01:21<00:00, 2086362.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to ./\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# Hyper parameters\n",
        "num_epochs = 4\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Download and prepare dataset\n",
        "# Transform them to tensors and normalise them\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "     ])\n",
        "\n",
        "# 2.2 Download data\n",
        "train_set = torchvision.datasets.CIFAR10(\"./\", train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(\"./\", train=False, download=True, transform=transform)\n",
        "\n",
        "# 2.3 Use DataLoader to get batches and shuffle\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Q1. Why are there 3 values in each list of the Normalize() function? What does each value and each list represent?\n",
        "# Each of the three input list means mean list, standard deviation list. \n",
        "# Since the given image tensor has three channels(shape.images : 4, 3, 32, 32, the '3' means number of channels of tensor), \n",
        "# the average value and standard deviation value required for applying normalization to each channel were entered in the form of a list with three elements.\n",
        "# From Torchvision Docs : mean-Sequence of means for each channel, std-Sequence of standard deviations for each channel."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DuhDiijV7CNT"
      },
      "source": [
        "### Inspect the Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "032BETSy6a2K",
        "outputId": "b12c95e4-b370-4242-db3e-00b0afe50c4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image values: \n",
            "[tensor([[[[ 0.8431,  0.8980,  0.9294,  ...,  0.9765,  0.9843,  0.9922],\n",
            "          [ 0.8902,  0.9137,  0.9137,  ...,  0.9765,  0.9843,  0.9922],\n",
            "          [ 0.9294,  0.8667,  0.8902,  ...,  0.9765,  0.9843,  0.9922],\n",
            "          ...,\n",
            "          [-0.0667,  0.1686, -0.2941,  ...,  0.9059,  0.7569,  0.8039],\n",
            "          [-0.2549,  0.0196, -0.5529,  ...,  0.8745,  0.7961,  0.8667],\n",
            "          [-0.4745, -0.0588,  0.1686,  ...,  0.9137,  0.9137,  0.9451]],\n",
            "\n",
            "         [[ 0.1216,  0.1922,  0.2157,  ...,  0.4588,  0.3961,  0.3647],\n",
            "          [ 0.1765,  0.1922,  0.1843,  ...,  0.4824,  0.4196,  0.3569],\n",
            "          [ 0.2000,  0.1373,  0.1686,  ...,  0.4902,  0.4510,  0.3804],\n",
            "          ...,\n",
            "          [-0.6863, -0.5686, -0.7255,  ...,  0.2078,  0.0118,  0.0510],\n",
            "          [-0.7569, -0.6549, -0.8745,  ...,  0.1373,  0.0510,  0.1373],\n",
            "          [-0.8431, -0.6392, -0.4588,  ...,  0.2078,  0.2000,  0.2392]],\n",
            "\n",
            "         [[-0.8118, -0.8275, -0.8118,  ..., -0.7098, -0.7412, -0.7647],\n",
            "          [-0.8275, -0.8431, -0.8353,  ..., -0.6941, -0.7333, -0.7725],\n",
            "          [-0.8039, -0.8510, -0.8353,  ..., -0.7098, -0.7333, -0.7725],\n",
            "          ...,\n",
            "          [-0.9529, -0.9686, -0.9373,  ..., -0.8039, -0.8902, -0.9059],\n",
            "          [-0.9373, -0.9451, -0.9529,  ..., -0.8510, -0.8980, -0.8824],\n",
            "          [-0.9373, -0.9373, -0.9216,  ..., -0.8353, -0.8510, -0.8196]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9294,  0.9137,  0.9216,  ...,  0.9216,  0.9137,  0.9137],\n",
            "          [ 0.9373,  0.9294,  0.9529,  ...,  0.9451,  0.9373,  0.9373],\n",
            "          [ 0.9373,  0.9373,  0.9608,  ...,  0.9608,  0.9529,  0.9529],\n",
            "          ...,\n",
            "          [ 0.8902,  0.9059,  0.9451,  ...,  0.9529,  0.9451,  0.9294],\n",
            "          [ 0.8902,  0.9137,  0.9451,  ...,  0.9451,  0.9373,  0.9294],\n",
            "          [ 0.8431,  0.8824,  0.9059,  ...,  0.9216,  0.9137,  0.9059]],\n",
            "\n",
            "         [[ 0.8902,  0.8745,  0.8824,  ...,  0.9294,  0.9216,  0.9216],\n",
            "          [ 0.8980,  0.8902,  0.9137,  ...,  0.9529,  0.9451,  0.9451],\n",
            "          [ 0.8980,  0.8980,  0.9216,  ...,  0.9686,  0.9608,  0.9608],\n",
            "          ...,\n",
            "          [ 0.8510,  0.8667,  0.9059,  ...,  0.9608,  0.9529,  0.9373],\n",
            "          [ 0.8510,  0.8745,  0.9059,  ...,  0.9529,  0.9451,  0.9373],\n",
            "          [ 0.8196,  0.8588,  0.8824,  ...,  0.9294,  0.9216,  0.9137]],\n",
            "\n",
            "         [[ 0.8431,  0.8275,  0.8353,  ...,  0.8667,  0.8588,  0.8588],\n",
            "          [ 0.8510,  0.8431,  0.8667,  ...,  0.8902,  0.8824,  0.8824],\n",
            "          [ 0.8431,  0.8510,  0.8745,  ...,  0.9059,  0.8980,  0.8980],\n",
            "          ...,\n",
            "          [ 0.8039,  0.8196,  0.8588,  ...,  0.8980,  0.8902,  0.8745],\n",
            "          [ 0.8039,  0.8275,  0.8588,  ...,  0.8902,  0.8824,  0.8745],\n",
            "          [ 0.7647,  0.8039,  0.8353,  ...,  0.8667,  0.8588,  0.8510]]],\n",
            "\n",
            "\n",
            "        [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "          ...,\n",
            "          [-0.9843, -0.9843, -0.9843,  ..., -0.6941, -0.5843, -0.9373],\n",
            "          [-0.9922, -1.0000, -0.9922,  ..., -0.5686, -0.5294, -0.8745],\n",
            "          [-1.0000, -0.9922, -0.9843,  ..., -0.8275, -0.7961, -0.9137]],\n",
            "\n",
            "         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "          ...,\n",
            "          [-0.9843, -0.9843, -0.9843,  ..., -0.6941, -0.5843, -0.9373],\n",
            "          [-0.9922, -1.0000, -0.9922,  ..., -0.5686, -0.5294, -0.8745],\n",
            "          [-1.0000, -0.9922, -0.9843,  ..., -0.8275, -0.7961, -0.9137]],\n",
            "\n",
            "         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "          ...,\n",
            "          [-0.9843, -0.9843, -0.9843,  ..., -0.6941, -0.5843, -0.9373],\n",
            "          [-0.9922, -1.0000, -0.9922,  ..., -0.5686, -0.5294, -0.8745],\n",
            "          [-1.0000, -0.9922, -0.9843,  ..., -0.8275, -0.7961, -0.9137]]],\n",
            "\n",
            "\n",
            "        [[[-0.1059, -0.1059, -0.0902,  ..., -0.0745, -0.0824, -0.0902],\n",
            "          [-0.0745, -0.0745, -0.0588,  ..., -0.0588, -0.0588, -0.0667],\n",
            "          [-0.0667, -0.0667, -0.0510,  ..., -0.0588, -0.0588, -0.0667],\n",
            "          ...,\n",
            "          [-0.0980, -0.0980, -0.0745,  ..., -0.4980, -0.5373, -0.5608],\n",
            "          [-0.0745, -0.0824, -0.0745,  ..., -0.6000, -0.6392, -0.6627],\n",
            "          [-0.0902, -0.0824, -0.0745,  ..., -0.6941, -0.7333, -0.7255]],\n",
            "\n",
            "         [[ 0.0824,  0.0824,  0.0980,  ...,  0.0745,  0.0510,  0.0431],\n",
            "          [ 0.0980,  0.0980,  0.1216,  ...,  0.0980,  0.0745,  0.0667],\n",
            "          [ 0.1059,  0.1059,  0.1137,  ...,  0.0980,  0.0824,  0.0745],\n",
            "          ...,\n",
            "          [-0.1686, -0.1686, -0.1451,  ..., -0.5608, -0.6000, -0.6314],\n",
            "          [-0.1451, -0.1451, -0.1373,  ..., -0.6706, -0.7176, -0.7412],\n",
            "          [-0.1608, -0.1529, -0.1451,  ..., -0.7647, -0.8118, -0.8039]],\n",
            "\n",
            "         [[ 0.2863,  0.2784,  0.2941,  ...,  0.2706,  0.2627,  0.2627],\n",
            "          [ 0.2941,  0.2941,  0.3098,  ...,  0.2941,  0.2784,  0.2706],\n",
            "          [ 0.2784,  0.2784,  0.2863,  ...,  0.2784,  0.2627,  0.2549],\n",
            "          ...,\n",
            "          [-0.3569, -0.3490, -0.3255,  ..., -0.7176, -0.7255, -0.7490],\n",
            "          [-0.3176, -0.3255, -0.3098,  ..., -0.8118, -0.8275, -0.8431],\n",
            "          [-0.3490, -0.3412, -0.3333,  ..., -0.8980, -0.9059, -0.8980]]]]), tensor([3, 2, 5, 0])]\n",
            "Length: 2\n",
            "Type: <class 'list'>\n",
            "torch.Size([4, 3, 32, 32])\n",
            "tensor([3, 2, 5, 0])\n"
          ]
        }
      ],
      "source": [
        "# Access the first data sample in the train_set using next(iter())\n",
        "batch = next(iter(train_loader))\n",
        "print(f'Image values: \\n{batch}')\n",
        "print(f'Length: {len(batch)}')\n",
        "print(f'Type: {type(batch)}')\n",
        "\n",
        "# This means the data contains image-label pairs\n",
        "# Unpack them\n",
        "images, labels = batch\n",
        "# Same as these two lines:\n",
        "# image = batch[0]\n",
        "# label = batch[1]\n",
        "\n",
        "print(images.shape)\n",
        "print(labels)\n",
        "\n",
        "# Q2. What is the range of the values for the normalised image pixels?\n",
        "# -1 to 1.\n",
        "\n",
        "# Q3. What does each index value of the shape of the image represent?\n",
        "# he tensor has a shape of (batch_size=4, channels=3, height=32, width=32).\n",
        "# In the context of image processing tasks, this shape indicates that the tensor represents a batch of 4 RGB images,\n",
        "# where each image has a height and width of 32 pixels. The channels dimension corresponds to the red, green, and blue channels of each image.\n",
        "\n",
        "# Q4. What do the label values represent?\n",
        "# It means the index of the class to which each image belongs. \n",
        "# Since four image data are stored in one batch, the index of the class corresponding to each of the four images is shown.\n",
        "# The index ranges from 0 to 9.\n",
        "# if index : tensor([6, 1, 4, 9]) -> 'frog', 'car', 'deer', 'truck'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "71_0Ci7gZkqV"
      },
      "source": [
        "### View some images\n",
        "- Note that images have been normalised and may not look very clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "Zwz0kGvfuL6d",
        "outputId": "3005dd05-c647-4448-de3c-1ddd892612af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class labels: tensor([3, 2, 5, 0])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f975a2298b0>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x1200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a grid \n",
        "plt.figure(figsize=(12,12))\n",
        "grid = torchvision.utils.make_grid(tensor=images, nrow=4) # nrow = number of images displayed in each row\n",
        "\n",
        "print(f\"class labels: {labels}\")\n",
        "\n",
        "# Use grid.permute() to transpose the grid so that the axes meet the specifications required by \n",
        "# plt.imshow(), which are [height, width, channels]. PyTorch dimensions are [channels, height, width].\n",
        "plt.imshow(grid.permute(1,2,0))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jVMti_g2J-W5"
      },
      "source": [
        "## CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tWePXqBQKAp7"
      },
      "outputs": [],
      "source": [
        "class Test(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5, padding=1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5) \n",
        "    # flatten 3D tensor to 1D tensor\n",
        "    self.fc1 = nn.Linear(400, 128) # Q8. Fill out the correct input dimensions \n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10) # final output matches num_classes\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Conv + ReLU + pool\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    out = self.conv1(x)\n",
        "    print(f'After Conv1: {out.shape}')\n",
        "    print(f'Padding: {self.conv1.padding}')\n",
        "    out = self.pool(F.relu(out))\n",
        "    print(f'After Pool1: {out.shape}')\n",
        "    out = self.conv2(out)\n",
        "    print(f'After Conv2: {out.shape}')\n",
        "    out = self.pool(F.relu(out))\n",
        "    print(f'After Pool2: {out.shape}')\n",
        "    # Flatten it before fc1\n",
        "    out = out.reshape(-1, 400) # Q8. Fill out the correct dimension after -1\n",
        "    print(f'Before fc1: {out.shape}')\n",
        "    out = self.fc1(out)\n",
        "    out = self.relu(out)\n",
        "    print(f'After fc1: {out.shape}')\n",
        "    out = self.fc2(out)\n",
        "    out = self.relu(out)\n",
        "    print(f'After fc2: {out.shape}')\n",
        "    out = self.fc3(out) # NO softmax as it will be included in CrossEntropyLoss\n",
        "    print(f'After fc3: {out.shape}')\n",
        "    return out\n",
        "\n",
        "\n",
        "model = Test().to(device)\n",
        "# Let's view the softmax output\n",
        "probs = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "# Q5. What do the three arguments of the first convolutional layer, conv1 represent (3,6,5)? \n",
        "# 3 : the number of input channels, since given image has 3 channels(RGB), the conv1 must have '3' on this argument \n",
        "# 6 : the number of filters, the output has a depth of number of filters because each filter produces one channel in the output.  \n",
        "# 5 : the kernal size\n",
        "\n",
        "# Q6. Explain the arguments of the second convolutional layer, conv2 (6, 16, 5) \n",
        "# 6 : the number of input channels\n",
        "# 16 : the number of filters, the output has a depth of number of filters because each filter produces one channel in the output.  \n",
        "# 5 : the kernal size\n",
        "\n",
        "# Q7. Figure out the convolved image size after conv1\n",
        "# Convolved image size = ((input_width - filter_size + 2 * padding) / stride) + 1\n",
        "# convolved image size = ((32 - 5 + 2*1) / 1) + 1 = 30\n",
        "\n",
        "# Q8. Figure out the input size to the first fcn layer and fill out the code above in init() and forward()\n",
        "# The shape of output after conv1, pool, conv2, pool, relu layer is (4, 16, 5, 5).\n",
        "# Since the second argument of reshape() specifies the total number of elements of feature image, in the tensor after reshaping to a 1D tensor,\n",
        "# the answer is 16*5*5 = 400."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EZk7fcriftFm"
      },
      "source": [
        "### Run through a sample batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW7t-qz-FjGp",
        "outputId": "bbd515a1-ed0d-47da-92ff-7fc6b0d4177f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([4, 10])\n",
            "Softmax outputs:\n",
            " tensor([[8.0758e-01, 7.3856e-03, 7.6887e-02, 4.0760e-03, 1.3486e-02, 2.2135e-03,\n",
            "         8.1445e-05, 8.0920e-02, 6.0576e-03, 1.3131e-03],\n",
            "        [1.8116e-03, 4.4795e-04, 2.7097e-02, 2.8127e-01, 1.0087e-01, 5.5771e-01,\n",
            "         1.9485e-02, 9.9171e-03, 1.1498e-03, 2.4194e-04],\n",
            "        [1.6491e-03, 2.4200e-04, 3.2177e-01, 4.4972e-02, 1.0562e-01, 4.6018e-01,\n",
            "         7.5634e-03, 5.6119e-02, 1.2196e-03, 6.6326e-04],\n",
            "        [3.5809e-02, 1.6886e-02, 4.8371e-02, 5.6453e-02, 3.4027e-01, 8.1828e-02,\n",
            "         1.1997e-02, 2.8365e-01, 3.4306e-02, 9.0436e-02]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "sample = next(iter(train_loader))\n",
        "\n",
        "images, labels = sample\n",
        "\n",
        "images = images.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "output = model(images)\n",
        "print(f'Output shape: {output.shape}')\n",
        "print(f'Softmax outputs:\\n {probs(output)}')\n",
        "\n",
        "\n",
        "# Q9. Explain the shape of the output after conv1\n",
        "# torch.Size([4, 6, 28, 28])\n",
        "# The batch size does not change. Because the batch size of the given image is 4, it remains the same.\n",
        "# The output has a depth of 'num_filters'=6 because each filter produces one channel in the output. \n",
        "# The height and width of the output are determined by the size of the filter (5x5), \n",
        "# the padding used (if any), and the stride used (in this case, stride=1). Using the formula \n",
        "# output image size = ((W - F + 2P) / S) + 1 as explained in the previous question, the output would have a size of 28x28.\n",
        "\n",
        "# Q10. What does the pooling do to the dimensions of the feature images here?\n",
        "# Since MaxPool2d with a pool size of 2x2 and stride of 2 is used, \n",
        "# the height and width of the output feature maps are half of the height and width of the input feature images, respectively.\n",
        "\n",
        "# Pooling operates on each channel of the input feature maps separately, taking the maximum value over a local neighborhood of pixels. \n",
        "# This reduces the spatial resolution of the feature maps, while preserving the most salient features. \n",
        "# This can help to reduce overfitting and improve computational efficiency of NN model.\n",
        "\n",
        "# Q11. Add padding=1 to conv1 and rerun the last two code cells. How did padding affect the dimensions of the feature images?\n",
        "# output image size = ((W - F + 2P) / S) + 1\n",
        "# After the conv1, the shape of output : torch.Size([4, 6, 30, 30])\n",
        "# ((32-5+2)/1)+1 = 30, the height and width of feature images increased than before\n",
        "\n",
        "# Q12. What is represented by each list returned by Softmax outputs?\n",
        "# The output of the Softmax function is a probability distribution over the classes in classification probelem.\n",
        "# Since there are 10 classes, it was returned in the form of a 2D tensor with a size(4*10) \n",
        "# indicating the logit value corresponding to each class of images in the batch, respectively.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eQj3gsf7Y6Ql"
      },
      "source": [
        "\n",
        "### Let's Train!\n",
        "- Now that we know and understand how CNNs work, let's put everything together for CIFAR-10 dataset\n",
        "  - Download the data in batches and normalisation with shuffling\n",
        "  - Build a model with 2 CNN layers containing ReLU and pooling\n",
        "  - Passing the feature images to 3 fully connected layers (FCNs) also containing RELU activation\n",
        "  - The final layer has 10 units to reprsent the number of output classes\n",
        "  - Use Binary Cross Entropy Loss and SGD optimiser\n",
        "  - Evaluate the model on the test data on EACH class\n",
        "\n",
        "**IMPORTANT!** Fill out the missing code below before training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-1_4tKL4X3WQ"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5) \n",
        "    # flatten 3D tensor to 1D tensor\n",
        "    self.fc1 = nn.Linear(400, 128) # TODO\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10) # final output matches num_classes\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Conv + ReLU + pool\n",
        "    out = self.pool(F.relu(self.conv1(x)))\n",
        "    out = self.pool(F.relu(self.conv2(out)))\n",
        "    # Flatten it before fc1\n",
        "    out = out.reshape(-1, 400) # TODO\n",
        "    out = F.relu(self.fc1(out))\n",
        "    out = F.relu(self.fc2(out))\n",
        "    out = self.fc3(out) # NO softmax as it will be included in CrossEntropyLoss\n",
        "    return out\n",
        "\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Q13. Use the Cross Entropy Loss for this task (UNCOMMENT & COMPLETE CODE BELOW)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Q14. Use the Stochastic Gradient Descent (SGD) optimiser, this time ADD momentum=0.9 (UNCOMMENT & COMPLETE CODE BELOW)\n",
        "import torch.optim as optim\n",
        "opt = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlg2FFaJKppP"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e15E85ZQKr1R",
        "outputId": "7de506d1-aba2-4c77-e196-c6ef4f869455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4, Iteration 1000/12500, Loss=2.3370 \n",
            "Epoch 1/4, Iteration 2000/12500, Loss=1.9753 \n",
            "Epoch 1/4, Iteration 3000/12500, Loss=2.0315 \n",
            "Epoch 1/4, Iteration 4000/12500, Loss=1.7766 \n",
            "Epoch 1/4, Iteration 5000/12500, Loss=1.0002 \n",
            "Epoch 1/4, Iteration 6000/12500, Loss=1.2944 \n",
            "Epoch 1/4, Iteration 7000/12500, Loss=1.4550 \n",
            "Epoch 1/4, Iteration 8000/12500, Loss=1.0454 \n",
            "Epoch 1/4, Iteration 9000/12500, Loss=1.5537 \n",
            "Epoch 1/4, Iteration 10000/12500, Loss=1.1972 \n",
            "Epoch 1/4, Iteration 11000/12500, Loss=0.7386 \n",
            "Epoch 1/4, Iteration 12000/12500, Loss=2.5292 \n",
            "Epoch 2/4, Iteration 1000/12500, Loss=0.7618 \n",
            "Epoch 2/4, Iteration 2000/12500, Loss=1.1494 \n",
            "Epoch 2/4, Iteration 3000/12500, Loss=1.4192 \n",
            "Epoch 2/4, Iteration 4000/12500, Loss=1.1864 \n",
            "Epoch 2/4, Iteration 5000/12500, Loss=1.3398 \n",
            "Epoch 2/4, Iteration 6000/12500, Loss=1.1420 \n",
            "Epoch 2/4, Iteration 7000/12500, Loss=0.3626 \n",
            "Epoch 2/4, Iteration 8000/12500, Loss=0.9433 \n",
            "Epoch 2/4, Iteration 9000/12500, Loss=0.8705 \n",
            "Epoch 2/4, Iteration 10000/12500, Loss=1.7990 \n",
            "Epoch 2/4, Iteration 11000/12500, Loss=1.1999 \n",
            "Epoch 2/4, Iteration 12000/12500, Loss=1.7178 \n",
            "Epoch 3/4, Iteration 1000/12500, Loss=1.6959 \n",
            "Epoch 3/4, Iteration 2000/12500, Loss=0.9038 \n",
            "Epoch 3/4, Iteration 3000/12500, Loss=1.1392 \n",
            "Epoch 3/4, Iteration 4000/12500, Loss=1.4385 \n",
            "Epoch 3/4, Iteration 5000/12500, Loss=1.8955 \n",
            "Epoch 3/4, Iteration 6000/12500, Loss=0.5996 \n",
            "Epoch 3/4, Iteration 7000/12500, Loss=1.8900 \n",
            "Epoch 3/4, Iteration 8000/12500, Loss=1.5216 \n",
            "Epoch 3/4, Iteration 9000/12500, Loss=1.2177 \n",
            "Epoch 3/4, Iteration 10000/12500, Loss=0.4221 \n",
            "Epoch 3/4, Iteration 11000/12500, Loss=0.6976 \n",
            "Epoch 3/4, Iteration 12000/12500, Loss=0.4738 \n",
            "Epoch 4/4, Iteration 1000/12500, Loss=1.8217 \n",
            "Epoch 4/4, Iteration 2000/12500, Loss=0.7499 \n",
            "Epoch 4/4, Iteration 3000/12500, Loss=1.1616 \n",
            "Epoch 4/4, Iteration 4000/12500, Loss=1.1594 \n",
            "Epoch 4/4, Iteration 5000/12500, Loss=0.1758 \n",
            "Epoch 4/4, Iteration 6000/12500, Loss=0.6309 \n",
            "Epoch 4/4, Iteration 7000/12500, Loss=0.8555 \n",
            "Epoch 4/4, Iteration 8000/12500, Loss=1.1377 \n",
            "Epoch 4/4, Iteration 9000/12500, Loss=0.3721 \n",
            "Epoch 4/4, Iteration 10000/12500, Loss=1.5381 \n",
            "Epoch 4/4, Iteration 11000/12500, Loss=0.6927 \n",
            "Epoch 4/4, Iteration 12000/12500, Loss=1.4333 \n"
          ]
        }
      ],
      "source": [
        "n_total_steps = len(train_set)\n",
        "n_iterations = -(-n_total_steps // batch_size) # ceiling division\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    #print(images.shape) # [4,3,32,32] batch size, channels, img dim\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass and Optimise\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # Print\n",
        "    if (i+1) % 1000 == 0:\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}, Iteration {i+1}/{n_iterations}, Loss={loss.item():.4f} ')\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HahXHRsSMo6H"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIuqYw8JMqy-",
        "outputId": "bb75cc87-b148-439c-a330-bca5eb9b30cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the WHOLE CNN = 10.0 %\n",
            "Accuracy of plane: 0.0 %\n",
            "Accuracy of car: 0.0 %\n",
            "Accuracy of bird: 0.0 %\n",
            "Accuracy of cat: 0.0 %\n",
            "Accuracy of deer: 100.0 %\n",
            "Accuracy of dog: 0.0 %\n",
            "Accuracy of frog: 0.0 %\n",
            "Accuracy of horse: 0.0 %\n",
            "Accuracy of ship: 0.0 %\n",
            "Accuracy of truck: 0.0 %\n"
          ]
        }
      ],
      "source": [
        "# Deactivate the autograd engine to reduce memory usage and speed up computations (backprop disabled).\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  n_class_correct = [0 for i in range(10)]\n",
        "  n_class_samples = [0 for i in range(10)]\n",
        "\n",
        "\n",
        "  # Loop through test set\n",
        "  for images, labels in test_loader:\n",
        "    # Put images on GPU\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    # Run on trained model\n",
        "    outputs = model(images) \n",
        "\n",
        "    # Get predictions\n",
        "    # torch.max() returns actual probability value (ignored) and index or class label (selected)\n",
        "    _, y_preds = torch.max(outputs, 1)\n",
        "    n_samples += labels.size(0) # different to FFNN\n",
        "    n_correct += (y_preds == labels).sum().item()\n",
        "\n",
        "    # Keep track of each class\n",
        "    for i in range(batch_size):\n",
        "      label = labels[i]\n",
        "      pred = y_preds[i]\n",
        "      if (label == pred):\n",
        "        n_class_correct[label] += 1\n",
        "      n_class_samples[label] += 1\n",
        "\n",
        "  # Print accuracy\n",
        "  acc = 100.0 * n_correct / n_samples\n",
        "  print(f'Test Accuracy of the WHOLE CNN = {acc} %')\n",
        "\n",
        "  for i in range(len(classes)):\n",
        "    acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "    print(f'Accuracy of {classes[i]}: {acc} %')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TKj_ZV-Dzzn"
      },
      "outputs": [],
      "source": [
        "# Q15. Why don't we need to reshape the input images when training and testing?\n",
        "# In PyTorch, Convolutional layers are designed to work with input images of any size. \n",
        "# When we define a convolutional layer, we specify the size of the filters and the stride, \n",
        "# and the layer automatically computes the output size of the convolutional operation based on the size of the input image.\n",
        "# This means that we can use images of different sizes during training and testing, as long as they have the same number of channels (e.g., RGB images have 3 channels). \n",
        "\n",
        "# Q16. Try to improve the model performance, e.g. by increasing the epochs, changing batch size, adding convolutions, etc.\n",
        "# Provide the code chunk showing the improved accuracy on the test set below. What changes did you make?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teqrOqg4GhzE",
        "outputId": "3b349916-9d81-4ccc-efe3-c401def9bc97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Test Accuracy of the WHOLE CNN = 62.82 %\n",
            "Accuracy of plane: 74.59016393442623 %\n",
            "Accuracy of car: 75.38461538461539 %\n",
            "Accuracy of bird: 56.36363636363637 %\n",
            "Accuracy of cat: 45.689655172413794 %\n",
            "Accuracy of deer: 53.125 %\n",
            "Accuracy of dog: 56.89655172413793 %\n",
            "Accuracy of frog: 72.53521126760563 %\n",
            "Accuracy of horse: 61.940298507462686 %\n",
            "Accuracy of ship: 75.53956834532374 %\n",
            "Accuracy of truck: 72.56637168141593 %\n",
            "Test Accuracy of the WHOLE CNN = 63.98 %\n",
            "Accuracy of plane: 67.21311475409836 %\n",
            "Accuracy of car: 84.61538461538461 %\n",
            "Accuracy of bird: 41.81818181818182 %\n",
            "Accuracy of cat: 38.793103448275865 %\n",
            "Accuracy of deer: 68.75 %\n",
            "Accuracy of dog: 71.55172413793103 %\n",
            "Accuracy of frog: 79.5774647887324 %\n",
            "Accuracy of horse: 69.40298507462687 %\n",
            "Accuracy of ship: 74.10071942446044 %\n",
            "Accuracy of truck: 70.79646017699115 %\n",
            "Test Accuracy of the WHOLE CNN = 62.48 %\n",
            "Accuracy of plane: 66.39344262295081 %\n",
            "Accuracy of car: 83.07692307692308 %\n",
            "Accuracy of bird: 49.09090909090909 %\n",
            "Accuracy of cat: 47.41379310344828 %\n",
            "Accuracy of deer: 55.46875 %\n",
            "Accuracy of dog: 56.03448275862069 %\n",
            "Accuracy of frog: 72.53521126760563 %\n",
            "Accuracy of horse: 62.6865671641791 %\n",
            "Accuracy of ship: 84.89208633093526 %\n",
            "Accuracy of truck: 68.14159292035399 %\n",
            "Test Accuracy of the WHOLE CNN = 60.66 %\n",
            "Accuracy of plane: 61.47540983606557 %\n",
            "Accuracy of car: 82.3076923076923 %\n",
            "Accuracy of bird: 38.18181818181818 %\n",
            "Accuracy of cat: 53.44827586206897 %\n",
            "Accuracy of deer: 53.90625 %\n",
            "Accuracy of dog: 56.03448275862069 %\n",
            "Accuracy of frog: 65.49295774647888 %\n",
            "Accuracy of horse: 63.43283582089552 %\n",
            "Accuracy of ship: 74.82014388489209 %\n",
            "Accuracy of truck: 64.60176991150442 %\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "batch_size = 8 # increased the batch size to get faster results\n",
        "learning_rate = 0.001\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "     ])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(\"./\", train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(\"./\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5) \n",
        "    # flatten 3D tensor to 1D tensor\n",
        "    self.fc1 = nn.Linear(400, 128) # TODO\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10) # final output matches num_classes\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Conv + ReLU + pool\n",
        "    out = self.pool(F.relu(self.conv1(x)))\n",
        "    out = self.pool(F.relu(self.conv2(out)))\n",
        "    # Flatten it before fc1\n",
        "    out = out.reshape(-1, 400) # TODO\n",
        "    out = F.relu(self.fc1(out))\n",
        "    out = F.relu(self.fc2(out))\n",
        "    out = self.fc3(out) # NO softmax as it will be included in CrossEntropyLoss\n",
        "    return out\n",
        "\n",
        "# optimizing number of epochs.\n",
        "num_epochs_list = [6, 10, 14, 18]\n",
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "n_total_steps = len(train_set)\n",
        "n_iterations = -(-n_total_steps // batch_size) # ceiling division\n",
        "\n",
        "for num_epochs in num_epochs_list:\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and Optimise\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        n_correct = 0   \n",
        "        n_samples = 0\n",
        "        n_class_correct = [0 for i in range(10)]\n",
        "        n_class_samples = [0 for i in range(10)]\n",
        "\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images) \n",
        "        _, y_preds = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0) \n",
        "        n_correct += (y_preds == labels).sum().item()\n",
        "        for i in range(batch_size):\n",
        "            label = labels[i]\n",
        "            pred = y_preds[i]\n",
        "        if (label == pred):\n",
        "            n_class_correct[label] += 1\n",
        "        n_class_samples[label] += 1\n",
        "\n",
        "    # Print accuracy\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Test Accuracy of the WHOLE CNN = {acc} %')\n",
        "\n",
        "    for i in range(len(classes)):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of {classes[i]}: {acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXVedMHUIR7Y"
      },
      "outputs": [],
      "source": [
        "# It showed the best performance when the number of epochs was 10."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
